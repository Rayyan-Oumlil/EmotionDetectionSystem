{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc8c90f",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbe4441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is cuda available?\n",
    "import torch\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf6de32",
   "metadata": {},
   "source": [
    "### Create dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a2754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_balanced_split(dataset, val_samples_per_class=None):\n",
    "    \"\"\"\n",
    "    Split dataset into train and validation sets with equal samples per class.\n",
    "    If val_samples_per_class is None, uses 10% of the smallest class.\n",
    "    \"\"\"\n",
    "    # Get all targets (class labels)\n",
    "    targets = np.array(dataset.targets)\n",
    "    \n",
    "    # Find the size of each class\n",
    "    class_sizes = []\n",
    "    for class_idx in range(len(dataset.classes)):\n",
    "        class_indices = np.where(targets == class_idx)[0]\n",
    "        class_sizes.append(len(class_indices))\n",
    "    \n",
    "    # If not specified, take 10% of the smallest class\n",
    "    if val_samples_per_class is None:\n",
    "        val_samples_per_class = int(min(class_sizes) * 0.1)\n",
    "    \n",
    "    print(f\"\\nTaking {val_samples_per_class} samples per class for validation\")\n",
    "    \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    # For each class, take equal number of samples\n",
    "    for class_idx in range(len(dataset.classes)):\n",
    "        # Get all indices for this class\n",
    "        class_indices = np.where(targets == class_idx)[0]\n",
    "        \n",
    "        # Shuffle indices for this class\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Take fixed number for validation\n",
    "        val_indices.extend(class_indices[:val_samples_per_class])\n",
    "        train_indices.extend(class_indices[val_samples_per_class:])\n",
    "    \n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaefd0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking 300 samples per class for validation\n",
      "Number of classes: 6\n",
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']\n",
      "Total images: 121605\n",
      "Training images: 119805\n",
      "Validation images: 1800\n",
      "Class to index mapping: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5}\n",
      "\n",
      "Class distribution in training set:\n",
      "  angry: 19675 images\n",
      "  disgust: 1880 images\n",
      "  fear: 20185 images\n",
      "  happy: 35775 images\n",
      "  neutral: 24525 images\n",
      "  sad: 17765 images\n",
      "\n",
      "Class distribution in validation set:\n",
      "  angry: 300 images\n",
      "  disgust: 300 images\n",
      "  fear: 300 images\n",
      "  happy: 300 images\n",
      "  neutral: 300 images\n",
      "  sad: 300 images\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "# Define data transformations for grayscale images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((48, 48)),  # Resize to 48x48\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for 1 channel\n",
    "])\n",
    "\n",
    "# Load the full dataset from the folder structure\n",
    "full_dataset = ImageFolder(root='train_preprocessed_augmented', transform=transform)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "train_indices, val_indices = create_balanced_split(full_dataset, 300)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Number of classes: {len(full_dataset.classes)}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "print(f\"Class to index mapping: {full_dataset.class_to_idx}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "train_targets = [full_dataset.targets[i] for i in train_indices]\n",
    "for class_idx, class_name in enumerate(full_dataset.classes):\n",
    "    count = train_targets.count(class_idx)\n",
    "    print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "# Print class distribution in validation set\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "val_targets = [full_dataset.targets[i] for i in val_indices]\n",
    "for class_idx, class_name in enumerate(full_dataset.classes):\n",
    "    count = val_targets.count(class_idx)\n",
    "    print(f\"  {class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72513403",
   "metadata": {},
   "source": [
    "### EmotionCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 3 pooling layers: 48 -> 24 -> 12 -> 6\n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
    "        self.bn_fc = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14bdb5",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5424d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def metrics(num_epochs, train_losses, valid_losses, train_accuracies, valid_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), valid_losses, label='Valid Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss overtime')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), valid_accuracies, label='Valid Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy overtime')\n",
    "    plt.savefig(\"metrics.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf316a",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e8c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def calculate_class_weights(full_dataset, train_indices):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on training set distribution\n",
    "    \n",
    "    Args:\n",
    "        full_dataset: Full dataset with targets\n",
    "        train_indices: Indices used for training\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Class weights\n",
    "    \"\"\"\n",
    "    # Get training labels\n",
    "    train_targets = [full_dataset.targets[i] for i in train_indices]\n",
    "    train_targets = np.array(train_targets)\n",
    "    \n",
    "    # Calculate class weights using sklearn\n",
    "    classes = np.unique(train_targets)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=train_targets\n",
    "    )\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    \n",
    "    # Print class distribution and weights\n",
    "    print(\"\\nClass distribution and weights:\")\n",
    "    print(\"-\" * 60)\n",
    "    for class_idx, class_name in enumerate(full_dataset.classes):\n",
    "        count = np.sum(train_targets == class_idx)\n",
    "        weight = class_weights[class_idx].item()\n",
    "        print(f\"{class_name:10s}: {count:6d} images, weight: {weight:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b7bc8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, model_name=\"emotionCNN\", num_epochs=10):\n",
    "    print(\"start training using \")\n",
    "    print(device)\n",
    "#     backbone_params = [p for name, p in model.named_parameters() if 'fc' not in name]\n",
    "#     classifier_params = [p for name, p in model.named_parameters() if 'fc' in name]\n",
    "\n",
    "#     optimizer = torch.optim.Adam([\n",
    "#         {'params': backbone_params, 'lr': learning_rate * 0.1},  # Lower for pretrained\n",
    "#         {'params': classifier_params, 'lr': learning_rate}       # Higher for new layer\n",
    "# ])\n",
    "    train_losses, valid_losses = [], []\n",
    "    best_valid_loss = 1000\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        # device = next(model.parameters()).device\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # print(images.dtype, device)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            # if (i + 1) % 10 == 0:\n",
    "            #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "            #           f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Évaluation sur valid_loader\n",
    "        model.eval()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        valid_loss = running_loss / len(valid_loader)\n",
    "        valid_accuracy = 100 * correct / total\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    " \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%\")\n",
    "        if best_valid_loss > valid_loss:\n",
    "            torch.save(model.state_dict(), f\"emotion_{model_name}_best.pth\")\n",
    "            print(f\"Saved best new validalition loss with a value of {valid_loss}\")\n",
    "            best_valid_loss = valid_loss\n",
    "            \n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), f'emotion_{model_name}_final.pth')\n",
    "    print(f'Model saved as emotion_{model_name}.pth')\n",
    "    metrics(num_epochs, train_losses, valid_losses, train_accuracies, valid_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31354a",
   "metadata": {},
   "source": [
    "### Reset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell has been removed - ResNet training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850e5e1",
   "metadata": {},
   "source": [
    "### EmotionCNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model architecture\n",
    "model = EmotionCNN(num_classes=7)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load('emotion_emotionCNN_continued_final.pth'))\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "class_weights = calculate_class_weights(full_dataset, train_indices)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, \"emotionCNN_continued\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5828d",
   "metadata": {},
   "source": [
    "### Reset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been removed - ResNet evaluation code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccbc5b",
   "metadata": {},
   "source": [
    "### EmotionCNN evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d653f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Emotion labels (must match your training order)\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Define the same transforms used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "def predict_single_image(model, image_path, device):\n",
    "    \"\"\"\n",
    "    Predict emotion for a single image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained EmotionCNN model\n",
    "        image_path: Path to the image\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        str: Predicted emotion label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "        image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            predicted_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        return emotion_labels[predicted_class]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def make_predictions_csv(model, test_folder, csv_template, output_csv, device):\n",
    "    \"\"\"\n",
    "    Make predictions for all images in CSV template and save results\n",
    "    \n",
    "    Args:\n",
    "        model: Trained EmotionCNN model\n",
    "        test_folder: Folder containing test images\n",
    "        csv_template: Path to template CSV file\n",
    "        output_csv: Path to save predictions\n",
    "        device: torch device\n",
    "    \"\"\"\n",
    "    # Read template CSV\n",
    "    df = pd.read_csv(csv_template)\n",
    "    \n",
    "    print(f\"Making predictions for {len(df)} images...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        image_filename = row['id']\n",
    "        image_path = os.path.join(test_folder, image_filename)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image not found: {image_path}\")\n",
    "            predictions.append('neutral')  # Default prediction\n",
    "            continue\n",
    "        \n",
    "        # Make prediction\n",
    "        emotion = predict_single_image(model, image_path, device)\n",
    "        \n",
    "        if emotion is None:\n",
    "            emotion = 'neutral'  # Default if prediction fails\n",
    "        \n",
    "        predictions.append(emotion)\n",
    "        \n",
    "        # Print progress every 100 images\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(df)} images...\")\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    df['emotion'] = predictions\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✓ Predictions saved to: {output_csv}\")\n",
    "    print(f\"✓ Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Print prediction distribution\n",
    "    print(\"\\nPrediction distribution:\")\n",
    "    emotion_counts = df['emotion'].value_counts()\n",
    "    for emotion, count in emotion_counts.items():\n",
    "        print(f\"  {emotion:10s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EmotionCNN(num_classes=7)\n",
    "model.load_state_dict(torch.load('emotion_emotionCNN_continued_final.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "\n",
    "# Make predictions\n",
    "make_predictions_csv(\n",
    "    model=model,\n",
    "    test_folder='test',  # Folder containing test images\n",
    "    csv_template='test_template.csv',\n",
    "    output_csv='predictions.csv',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Done! Submit 'predictions.csv' to the competition.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
